{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma introdução à inferência com BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte:\n",
    "* [Hugging Face Course](https://huggingface.co/course/chapter2/1?fw=pt)\n",
    "* [An introduction to inference with BERT](https://github.com/BramVanroy/bert-for-inference/blob/master/introduction-to-bert.ipynb)\n",
    "* [Introdução ao BERT - Gerando sentence embeddings](https://github.com/HAILab-PUCPR/introducao-bert/blob/main/introducao-bert.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse notebook mostra exemplos de como usar o BERT para extrair embedding de sentenças. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanto bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de deep learning usam tensores. Tensores são basicamente vetores e vetores são basicamente um conjunto de números. O tokenize é responsável por **transformar** o texto em um tipo de dado que pode ser **pré-processado** pelo modelo. Modelos podem apenas processarnúmeros, então os tokenizers precisam converter o input de texto para um dado número.\n",
    "\n",
    "Existem vários algoritmos de tokenização, alguns deles são:\n",
    "\n",
    "* **Word-based**: Separa o texto em palavras  encontra uma representação numérica pra cada uma delas.\n",
    "* **Character-based**: Separa o texto em caracteres, em vez de palavras.\n",
    "* **Subword tokenization**: Baseiam-se no princípio de que palavras usadas com frequência  não devem ser dividias em subpalavras menores, mas palavras raras devem ser decompostas em subpalavras significativas.\n",
    "\n",
    "O BERT usa o algoritmo de tokenização chamado **WordPiece** que é baseado em **Subword tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5fd814aa444662b0bb46c36e37beea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f04ab96c6394711a5a06f4135a9825f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76b9a65c594c65b6990071cf536d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54a41e6873247eb9136abb40c9868c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the tokenizer with a pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durando o pré-treinamento, o tokenizer foi \"treinado\" também, gerando um vocabulário conhecido. Cada palabra é associado a um index (um número) e essa número pode ser usado no modelo. Para lidar com palavras que o tokenizer não conhece ainda (out-of-vocabulary or OOV), uma técnica especial é usada para garantir que o tokenizer aprendeu \"subword units\". Isso significa que modelos pré-treinados não terão problemas de OOV. Quando o tokenizer não reconhece uma palavra (que não está no vocabulário) ele vai tentar separar essa palavra em partes pequenas que ele conhece. O tokenizer do BERT que usa WordPiece para separar os tokens. Exemplo: a palabra `granola` é separada em `gran` e `##ola` onde `##` indica o início da substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granola_ids [101, 12604, 6030, 6963, 102]\n",
      "type of granola_ids <class 'list'>\n",
      "granola_tokens ['[CLS]', 'gran', '##ola', 'bars', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Convert the string \"granola bars\" to tokenized vocabulary IDs\n",
    "granola_ids = tokenizer.encode('granola bars')\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))\n",
    "# Convert the IDs to the actual vocabulary item\n",
    "# Notice how the subword unit (suffix) starts with \"##\" to indicate \n",
    "# that it is part of the previous string\n",
    "print('granola_tokens', tokenizer.convert_ids_to_tokens(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber os \"tokens espciais\" [CLS] e [SEP]. Esses tokens são adicionados automaticamente pelo método `.encode()`. O primeiro é um token de classificação que já vem pré-treinado. É especificamente inserido para qualquer tipo de tarefa de classificação. Então, em vez de ter a média de todos os tokens e usar isso como representação da frase, é recomendado apenas pegar a saída do [CLS] que então representa a frase inteira. [SEP], por outro lado, é inserido como um separador entre várias instâncias. É usado como a previsão da próxima frase, onde é um separador entre a frase atual e a próxima. É especialmente importante lembrar que o toke [CLS] desempenha um grande papela nas tarefas de classificação e regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos quase o o tipo certo de data para começar. Como vimos, o tipo de dado dos IDs de cada token é uma lista de inteiros. Vamos usar a biblioteca `transformers` junto com o PyTorch, que trabalha com tensores. Um tensor é um tipo especial de lista otimizada que normalmente é usada em deep learning. Para converter os IDs dos tokens em um tensor é só passar a lista de IDs no construtor do tensor. Aqui é usado um `LongTensor` que usado para inteiros. Para números de ponto flutuante é usado `FloatTensor` ou só `Tensor`. O método `.encode()` do tokenizer pode retornar um tensor ao em vez de uma lista passando o parâmetro `return_tensors='pt'`mas para ilustrar, vamos apenas fazer a conversão manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granola_ids tensor([  101, 12604,  6030,  6963,   102])\n",
      "type of granola_ids <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of IDs to a tensor of IDs \n",
    "granola_ids = torch.LongTensor(granola_ids)\n",
    "# Print the IDs\n",
    "print('granola_ids', granola_ids)\n",
    "print('type of granola_ids', type(granola_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que o input foi pré-processado em um tensor de IDs (lembrando que cada valor de ID corresponde ao ID do token no vocabulário criado pelo tokenizador). O modelo sabe qual palavra está sendo processada porque ele sabe qual token pertence a determinado ID. No BERT e na maioria dos modelos de linguagem baseados em Transformer, a primeira camada é uma camada de *embbeding*, cada token possu um *embedding* relacionado. No BERT, o *embedding* de um token é a soma de três tipos de *embeddings*: o *embedding* to token (gerado para o próprio *token*), o *embedding* do segmento (indica se o segmento faz parte da primeira ou da segunda sentença, não usado na inferência de uma única sentença) e o *embedding* de posição (distingue a posição do token na sentença).\n",
    "\n",
    "Abaixo, uma imagem do BERT retirada do artigo publicado.\n",
    "\n",
    "![enter image description here](https://github.com/BramVanroy/bert-for-inference/blob/master/img/bert-embeddings.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniciando o modelo\n",
    "\n",
    "Primeiramente é preciso iniciar o modelo, assim como o tokenizer, o modelo já é pré-treinado, o que  nos permite usar um modelo de linguagem já pré-treinado para obter representações de *token* ou de sentenças.\n",
    "\n",
    "Vamos usar o mesmo modelo pré-treinado usado no tokenizer (`bert-base-uncased`). Ele é o menor modelo BERT que já foi treinado em textos com letras minúsculas. Como o modelo foi treinado com letras minúsculas, ele não sabe sobre textos com letras maiúsculas. O tokenizer automaticamente deixa o texto em letras minúsculas. Usar textos com letras minúsculas ou maiúsculas depende da tarefa. NER, por exemplo, podem requerer modelos treinados com maiúsculas e minúsculas (neste caso, troque \"uncased\" por \"cased\").\n",
    "\n",
    "No exemplo abaixo, um argumento foi adicionado e passado para o iniciar o modelo. `output_hidden_states` dá informações de saída. Por padrão, o `BERTModel` vai retornar uma tupla mas o conteúdo dessa tupla é diferente dependendo da configuração do modelo. Quando passado `output_hidden_states=True`, a tupla vai conter:\n",
    "\n",
    "1. O último estado oculto (`batch_size, sequence_length, hidden_size`)\n",
    "2. `pooler_output` do token de classificação (`batch_size, hidden_size`)\n",
    "3. os estados_ocultos das saídas do modelo em cada camada e as saídas dos embeddings iniciais (`batch_size, sequence_length, hidden_size`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU x CPU\n",
    "\n",
    "As placas gráficas (GPUs) são muito melhores em fazer operações em tensores do que uma CPU, portanto, sempre que disponível, executaremos os cálculos em GPU, como a CUDA (para isso, precisaremos de uma versão torch compatível com GPU.)\n",
    "\n",
    "Assim, movemos nosso modelo para o dispositivo correto: se estiver disponível, moveremos o modelo `.to()` à GPU, caso contrário, permanecerá na CPU. É importante lembrar que o modelo e os dados a serem processados precisam estar no mesmo dispositivo.\n",
    "\n",
    "Finalmente, definimos o modelo para o modo de avaliação (`.eval`), em contraste com o modo de treinamento (`.train()`). Na avaliação, não temos por exemplo o *dropout*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cea5f442c7415fb451860c0c6be005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "granola_ids = granola_ids.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferência\n",
    "\n",
    "O modelo foi inicializado e a string de entrada (\"granola_id\") foi convertida em um tensor. Os modelos de linguagem (como o `BERTModel`, usado acima) possuem um método `foward()`, chamado automaticamente ao chamar o objeto. Esse método envia o tensor de entrada para frente do modelo e retorna a saída.\n",
    "\n",
    "Como aqui trata-se de inferência, e não do treinamento ou ajuste (`fine-tuning`) do modelo, esta é a única etapa que chamamos o modelo esperando uma saída (`output`). Portanto, não precisamos otimizar o modleo para calcular gradientes e fazer o `backpropagation`.\n",
    "\n",
    "Definimos `torch.no_grad()` na inferência para informar ao modelo que não faremos nenhum cálculo de gradiente e/ou retropropagação, tornando a inferência mais rápida e mais eficiente em termos de memória.\n",
    "\n",
    "Geralmente, os métodos `model.eval()` e `torch.no_grad()` são usados juntos para avaliar e testar o modelo. Para treinar o modelos usamos o método `model.train()` e o método `torch.no_grad()` **não** deve ser usado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lote (batch)\n",
    "\n",
    "Abaixo, veremos um método chamado `.unsqueeze()`, que \"descomprime\" um tensor adicionando uma dimensão extra. Então, nosso tensor de granola de tamanho `(7,)` irá se transformar em um tensor de `(1, 7)`, onde `1` é a dimensão da frase. Essas suas dimensões são requeridas pelo modelo: ele é otimizado para treinar em **lotes** (batches), como veremos adiante.\n",
    "\n",
    "Um lote consistem em vários textos de entrada \"ao mesmo tempo\" (geralmente em potêcia de dois, por exemplo, 64). Com um tamanho de lote de 64 (ou seja, 64 frases de uma vez), o tamanho do lote seria `(64, n)` onde `64` é o número de frases e `n` o comprimento da sequência. Aqui, onde usamos apenas uma entrada, isso não é importante, mas ao ajustar o modelo, precisamos trabalhar com lotes, pois o cálculo do gradiente será melhor para grandes lotes.\n",
    "\n",
    "Nesses casos, `n` precisa ser o mesmo para todas as entradas, ou seja, nõa é possível ter uma sequência de 7 itens e uma de 12 itens (para lidar com isso, usamos técnicas de *paddind*). O tamanho de entrada do modelo precisa ser `(n_input_sentences, seq_len)` onde `seq_len` pode ser determinado de diferentes maneiras.\n",
    "\n",
    "Duas escolhas populares são: usar o texto mais longo do lote como `seq_len` (por exemplo, 12) e preencher textos curtos até esse comprimento, ou definir um comprimento de sequência máximo fixo para o modelo (normalmente 512) e preencher todos os itens até este mesmo comprimento. A última abordagem é mais fácil de implementar, mas não é eficiente em termos de memória e é computacionalmente mais pesada. Fica a seu critério."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([1, 5])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(granola_ids.size())\n",
    "# descomprimir IDs para obter o tamanho do lote = 1 como dimensão extra\n",
    "granola_ids = granola_ids.unsqueeze(0)\n",
    "print(granola_ids.size())\n",
    "\n",
    "print(type(granola_ids))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=granola_ids)\n",
    "\n",
    "# a saída é uma tupla\n",
    "print(type(out))\n",
    "# a tupla contém três elementos, que serão explicados abaixo\n",
    "print(len(out))\n",
    "# aqui serão listados apenas os estados ocultos do modelo (hidden_states)\n",
    "hidden_states = out[2]\n",
    "##print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estado oculto (hidden state)\n",
    "\n",
    "Como visto acima, nós enviamos os IDs de nossos tokens de entrada por meio do método `model()`, que chama internamente o método `foward()`. O `out` é uma tupla com todos os itens de saída relevantes, sendo o terceiro o mais importante, pois contém os estados ocultos (`hidden_states`) do modelo após a execução de um *foward*.\n",
    "\n",
    "`hidden_states` é uma tupla de saída de cada camada no modelo para cada token. na execução anterior, vimos que cada tupla contém 13 itens. Quando você executa `print(model)`, a arquitetura do BertModel é exibida(todas as camadas, de cima pra baixo). O `hidden_states` inclui a saída da camada `embeddings` e a saída de todos os 12 `BERTLayer` no codificador. A saída de cada camada tem um tamanho de `(batch_size, sequence_length, 768)`.\n",
    "\n",
    "Em nosso exemplo, isso é `(1, 7, 768)` porque temos apenas uma string de entrada (tamanho do lote = 1), e nossa string de entrada foi tokenizada em sete IDs (comprimento de sequência de 7). `768` é o número de dimensões ocultas.\n",
    "\n",
    "Como podemos ver, há mais uma camada após o codificador, chamada pooler, que não faz parte dos hidden_states. Esta camada é usada para \"agrupar\" a saída do token de classificação. Sua saída é retornada no segundo item da tupla de saída out, conforme visto antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporação de sentença (sentence embeddings)\n",
    "\n",
    "Agora que temos todos os `hidden_states`, podemos utilizá-lo em algumas tarefas. Por exemplo, para recuperar uma incorporação de frases (*sentence embeddings*) calculando a média de todos os tokens. Ou seja, vamos reduzir o tamanho de `(1, 7, 768)` para `(1, 768)` onde `1` é o tamanho do lote e `768` é o número de dimensões ocultas.\n",
    "\n",
    "Há diversas maneiras de fazer uma abstração de frase de tokens, dependendo da tarefa de NLP. Aqui, estamos usando a média. Por enquanto, usaremos apenas a saída da última camada do codificador, isto é, hidden_states `[-1]`. É importante indicar que queremos pegar o torch.meansobre um determinado eixo. Uma vez que o tamanho da saída das camadas é `(1, 7, 768)`, queremos fazer a média sobre os sete tokens, que estão na segunda dimensão `(dim = 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7497e-01,  1.8313e-01, -8.8651e-02,  2.1698e-01,  3.1942e-01,\n",
      "        -1.1412e-01,  7.4039e-02,  3.7655e-01, -4.1821e-01,  9.9971e-02,\n",
      "        -9.0242e-02, -2.4298e-01,  1.5542e-01,  4.2042e-01, -2.5547e-01,\n",
      "         2.9753e-01, -2.9643e-01, -2.5810e-02,  8.5306e-02,  1.0182e-01,\n",
      "         3.0401e-01, -4.4263e-01,  3.1249e-02,  1.4435e-01,  3.0189e-01,\n",
      "         7.3913e-02, -2.5580e-01,  3.1384e-01, -1.4688e-01, -1.5202e-01,\n",
      "         7.0785e-02,  4.0448e-01, -1.1769e-01,  3.1848e-01,  2.8021e-02,\n",
      "        -1.6934e-01,  3.5639e-01, -2.2931e-01, -1.1899e-01, -1.1182e-01,\n",
      "        -1.6003e-01,  7.9355e-02,  5.1107e-01,  5.2223e-02, -1.5481e-01,\n",
      "         2.8228e-02, -1.4365e-01, -4.7737e-01, -5.6638e-01, -4.8802e-01,\n",
      "        -1.1429e-01,  2.8087e-01, -5.7160e-02,  2.3862e-01,  3.5440e-01,\n",
      "         5.8237e-01,  1.2777e-01,  1.0363e-01,  3.0538e-01,  2.0989e-01,\n",
      "         1.1693e-01,  2.6346e-01, -1.5832e-01, -1.1380e-01,  1.7189e-02,\n",
      "        -3.4662e-02,  1.1470e-01,  3.2023e-02, -1.9781e-01, -1.2561e-01,\n",
      "        -4.8289e-02, -2.4562e-01,  5.0643e-03, -2.4147e-02,  2.2932e-01,\n",
      "        -1.9112e-01, -4.4624e-01,  8.6932e-02, -6.9348e-02, -2.6828e-01,\n",
      "         3.0473e-01,  3.3020e-01,  1.4786e-01,  3.7107e-01, -7.5337e-02,\n",
      "         3.3730e-01, -2.6694e-01, -1.1731e-01,  7.5014e-02,  4.6462e-01,\n",
      "        -1.4554e-01,  2.2036e-02,  3.7995e-01,  3.0877e-01,  3.8794e-01,\n",
      "        -5.0270e-02, -8.4905e-02, -1.6390e-01,  5.2378e-01,  1.8881e-01,\n",
      "         2.1466e-02, -8.5598e-02,  2.4094e-01,  3.2501e-01, -2.4542e-01,\n",
      "        -2.2705e-01, -2.5746e-01,  2.9763e-01, -1.7600e-02, -8.6889e-01,\n",
      "         1.7624e-01,  2.9194e-02,  7.0749e-02, -2.1513e-01, -3.4476e-01,\n",
      "        -1.7142e-01,  3.5134e-01,  3.5487e-01, -1.6037e-01,  2.3332e-01,\n",
      "        -1.6640e-01,  1.5522e-02, -1.3603e-01,  9.8263e-01, -1.6572e-01,\n",
      "         8.6756e-02,  1.7275e-02,  3.1124e-02,  4.5487e-01,  9.1419e-02,\n",
      "        -1.2291e-01,  2.6948e-01,  7.5361e-02,  8.1167e-02, -2.8039e-01,\n",
      "         8.6069e-02,  2.8712e-01, -3.2012e-01, -1.0920e-01, -3.4677e-01,\n",
      "        -2.3220e-01,  1.7452e-01, -8.9247e-01, -3.8039e-01,  1.6933e-01,\n",
      "        -4.0088e-02,  7.6205e-02,  2.0971e-01,  4.3188e-01, -5.6367e-02,\n",
      "         4.2553e-01,  6.6853e-02, -4.0933e-01,  2.4338e-01, -9.9158e-02,\n",
      "        -1.1955e-01, -3.4792e-01, -7.9455e-02,  2.1929e-01,  3.0778e-01,\n",
      "         3.1283e-01,  8.8517e-02,  1.4369e-01,  1.2768e-02, -2.3661e-01,\n",
      "        -2.5115e-01, -1.8090e-02,  9.1525e-02,  2.6672e-03,  3.7868e-01,\n",
      "        -1.7585e-01, -1.7576e-01,  4.4584e-01, -3.5462e-01, -3.4920e-01,\n",
      "         4.2924e-02,  1.1444e-02,  5.6267e-01,  5.9516e-03,  4.5876e-02,\n",
      "        -1.8924e+00,  6.9179e-02,  4.4243e-01, -1.2978e-02,  3.2357e-01,\n",
      "        -5.8833e-03,  5.9261e-01, -6.0558e-01,  1.8617e-01, -3.9271e-01,\n",
      "         2.7042e-01, -3.7876e-01, -2.2174e-01,  1.2152e-01,  3.6275e-01,\n",
      "        -1.9326e-01,  6.0437e-02, -3.5042e-01,  1.2261e-01, -4.3831e-02,\n",
      "        -1.9312e-01,  1.5187e-01,  1.3780e-01, -9.7899e-02, -3.7317e-01,\n",
      "         1.2430e+00,  3.2932e-01, -1.9085e-01,  2.1563e-01, -1.0158e-01,\n",
      "        -1.9931e-01,  4.1641e-01,  7.6540e-02, -2.9979e-01, -6.5525e-03,\n",
      "        -7.7947e-02,  4.2822e-03, -5.3286e-02, -2.8932e-01, -3.4237e-01,\n",
      "         2.9741e-01, -7.0199e-02, -4.5928e-01,  2.9630e-01, -2.5724e-01,\n",
      "        -1.6242e-01, -8.9294e-02,  1.3588e-01,  1.9840e-01,  7.3456e-02,\n",
      "         2.5921e-01,  1.7004e-01,  1.0998e-01,  2.1080e-01, -3.7167e-01,\n",
      "         8.9602e-02, -2.4095e-02,  3.9413e-02,  9.6415e-02, -3.9217e-01,\n",
      "        -1.8365e-01,  1.1395e-01,  4.8770e-01,  1.1218e-03, -1.9569e-01,\n",
      "        -1.2228e-02,  3.1669e-01,  2.1529e-01,  2.5503e-01, -2.3742e-01,\n",
      "         3.5058e-02, -8.1064e-01,  2.4614e-03, -2.6651e-01,  4.3942e-01,\n",
      "         1.9389e-02,  6.3593e-02, -2.3803e-01,  9.2668e-02,  1.8258e-01,\n",
      "        -3.8494e-02,  2.3593e-01,  5.7420e-01, -2.0413e-01, -4.4064e-01,\n",
      "         5.3445e-02, -1.5976e-01,  1.8722e-02, -1.0571e-01,  1.6751e-01,\n",
      "        -5.6544e-02,  3.5808e-02, -1.6339e-01, -1.2839e+00, -1.0260e-01,\n",
      "        -6.7205e-02,  1.2519e-01,  9.1995e-02, -4.3033e-02, -7.7112e-02,\n",
      "        -1.9279e-01,  4.8786e-01, -4.6132e-02,  2.3583e-01, -5.8109e-01,\n",
      "        -3.5453e-01, -1.2584e-01, -1.9348e-01, -1.5997e-02,  8.0767e-02,\n",
      "         7.3320e-03,  3.1523e-01, -4.2058e-01, -2.6260e-01, -2.7528e-02,\n",
      "        -1.1586e-01,  5.3806e-03,  4.3207e-01,  2.3536e-01, -5.6140e-01,\n",
      "        -9.4499e-02, -1.6396e-01,  1.5114e-01,  2.6866e-01, -5.2128e-02,\n",
      "         2.1231e-01, -1.9071e-02, -3.1587e-01, -3.1075e+00, -9.8635e-02,\n",
      "        -2.5031e-01, -4.1425e-01,  2.7539e-01, -5.0086e-01, -1.8764e-01,\n",
      "        -1.1095e-01, -3.1184e-01, -1.1116e-01, -3.4540e-01, -2.2962e-01,\n",
      "         2.4196e-01,  2.2008e-01,  2.9891e-01, -1.4837e-01,  2.3513e-01,\n",
      "        -1.4459e-01, -7.1072e-02,  3.1104e-01,  8.4301e-02, -4.1443e-01,\n",
      "         1.9445e-01, -1.7383e-01,  1.0017e-01,  3.1795e-01, -2.7422e-01,\n",
      "         3.4965e-01, -4.7570e-01, -1.1109e-01, -4.1904e-01,  1.8018e-01,\n",
      "        -3.4424e-01, -4.1224e-01,  3.8817e-01,  5.1788e-02, -5.0654e-01,\n",
      "         1.2701e-01,  5.8633e-01, -7.6472e-02,  1.5943e-01,  4.0271e-01,\n",
      "        -1.7491e-01,  8.8954e-02,  4.1438e-01,  7.0470e-02, -2.2776e-01,\n",
      "        -5.2665e-01, -4.8418e-02,  4.7993e-01,  2.6683e-01,  5.4173e-02,\n",
      "         3.4134e-01, -2.9446e-02,  2.8612e-01, -1.2683e-01,  3.2365e-01,\n",
      "         1.5207e-02,  7.5879e-02, -2.1957e-01,  4.3722e-01, -1.9480e-01,\n",
      "        -1.4549e-01,  2.1158e-01, -1.9193e-01,  8.2931e-02, -2.9274e-01,\n",
      "        -3.1519e-02,  8.2016e-02,  1.0614e-02, -2.1728e-01, -2.4810e-01,\n",
      "         8.3924e-02, -9.3397e-01,  9.7814e-02, -8.4303e-02, -1.4857e-01,\n",
      "        -5.1369e-02,  3.3001e-01,  1.8646e-01,  1.0419e-01, -2.0374e-01,\n",
      "        -3.7991e-02, -1.5000e-01,  4.2839e-02, -1.1730e-01, -4.0050e-01,\n",
      "        -2.3397e-01, -7.2866e-01, -3.5928e-01,  4.2980e-01,  2.0247e-01,\n",
      "         4.7776e-02, -1.8988e-01,  1.4026e-01,  4.7262e-01,  2.9465e-01,\n",
      "        -4.6721e-01,  4.3352e-02, -1.1868e-01, -7.1534e-01, -3.5330e-01,\n",
      "         6.3016e-01, -4.8528e-01, -3.8960e-02, -2.2114e-01, -2.4189e-01,\n",
      "        -6.2859e-02, -1.5963e-01, -1.5015e-01,  2.2635e-01, -2.9310e-01,\n",
      "         1.4918e-01, -2.4853e-01,  1.8756e-01, -1.7818e-01,  1.5789e-02,\n",
      "         8.9519e-01, -9.1420e-02,  3.9348e-01,  3.9633e-02,  4.3769e-01,\n",
      "         8.2129e-02, -2.6583e-01,  1.0719e-01,  2.4877e-01,  4.1521e-01,\n",
      "        -3.0047e-01,  1.6897e-01, -1.7872e-01, -1.7884e-01, -3.6050e-02,\n",
      "         6.8344e-02, -3.4804e-01, -9.3207e-02, -3.3487e-02, -1.9079e-01,\n",
      "        -4.7730e-02,  2.1589e-01, -1.7649e-02,  1.0272e-01,  2.2214e-01,\n",
      "         2.4598e-02,  8.8886e-02, -1.9065e-02,  4.7940e-01,  1.7633e-01,\n",
      "         4.2462e-02,  4.0794e-01,  1.7895e-01, -3.6609e-01, -2.2922e-01,\n",
      "         1.1337e-01, -5.0475e-01,  2.0761e-01, -5.2469e-01,  2.5814e-01,\n",
      "         2.0181e-01, -2.5736e-02, -2.3437e-01, -4.9483e-02,  9.1705e-02,\n",
      "        -6.4364e-02,  1.2149e-01, -4.1186e-01,  9.7710e-02, -1.2508e-01,\n",
      "         4.0764e-02, -3.4717e-01,  3.8143e-01,  7.6530e-02, -1.9414e-01,\n",
      "         1.8410e-01, -1.1920e-02, -2.0051e-01,  1.1443e-01,  1.8088e-01,\n",
      "        -1.7253e-01, -1.7402e-01,  1.1461e-01, -2.6830e-01, -9.8872e-02,\n",
      "         3.9775e-02,  1.3449e-01, -1.6283e-01,  2.1161e-03, -7.8610e-02,\n",
      "        -4.6200e-01,  5.7786e-01,  1.1279e-01,  1.9048e-01, -3.5723e-02,\n",
      "         2.1164e-01, -1.4711e-02, -3.3993e-01, -1.2703e-01,  1.0326e-01,\n",
      "         5.0948e-03,  3.7435e-01,  1.7189e-01,  5.8345e-01,  1.6680e-02,\n",
      "        -1.5684e-01,  1.5378e-01,  2.2153e-01, -2.6531e-01,  2.5017e-01,\n",
      "         4.2574e-01, -1.5066e-01,  2.1265e-01,  1.1999e-01, -4.2866e-01,\n",
      "         2.5088e-02,  2.5251e-01,  1.4382e-01,  2.3328e-01,  2.0313e-01,\n",
      "         1.7169e-01,  2.3621e-02,  5.1524e-02, -4.0105e-02, -2.4421e-01,\n",
      "        -3.3788e-01, -2.7906e-01,  8.5631e-02,  3.7364e-01,  3.3528e-01,\n",
      "        -2.2078e-01, -4.9876e-03,  4.2258e-02,  1.6184e-01,  2.2989e-01,\n",
      "         1.0930e-01,  9.5693e-02,  5.9743e-01,  6.8596e-01, -3.4541e-01,\n",
      "        -3.4690e-02, -4.9943e-02, -4.3827e-02, -9.8949e-03, -1.8422e-02,\n",
      "         6.7103e-02, -2.2258e-02, -2.2984e-02, -4.0703e-01,  2.6124e-01,\n",
      "         4.1154e-01, -2.2699e-02, -1.9614e-01, -9.2900e-02, -7.1759e-02,\n",
      "        -1.3627e-01, -2.9177e-01, -9.5442e-03, -2.9971e-01, -2.9885e-02,\n",
      "         4.1862e-01,  5.2153e-02,  6.1977e-02, -1.9548e-01,  1.2748e-01,\n",
      "         1.7454e-02,  2.9819e-01, -1.5988e-01,  1.5838e-02, -8.1304e-02,\n",
      "         1.8233e-02,  7.7923e-02,  1.7629e-01,  1.3150e-01, -1.5991e-01,\n",
      "         2.5084e-01,  2.5934e-01, -2.0483e-01,  3.1098e-02,  3.3086e-01,\n",
      "        -5.5236e-01, -1.7119e-02, -1.7480e-01,  5.3123e-01,  3.7780e-01,\n",
      "         1.3010e-01, -4.2269e-02,  2.4855e-01, -3.1270e-01, -5.0033e-01,\n",
      "         3.0209e-01, -5.2464e-01,  2.9340e-03,  9.4141e-01, -1.9262e-01,\n",
      "         1.2447e-02, -5.6114e-02, -3.5636e-01,  1.8700e-01, -4.5735e-02,\n",
      "         7.4748e-02, -2.0243e-01,  2.5534e-01, -3.2713e-01,  3.1265e-01,\n",
      "        -3.5376e-01, -5.1488e-02,  1.4935e-01, -3.9786e-02,  1.0336e-02,\n",
      "        -5.6745e-01, -8.2600e-02, -2.4669e-01, -1.2936e-01, -2.3212e-01,\n",
      "         7.3379e-02,  4.0745e-01, -1.6200e-01, -7.2631e-02,  1.1524e-01,\n",
      "        -1.4988e-01, -7.2060e-02,  5.8054e-01, -1.8932e-02,  1.1245e-01,\n",
      "         3.2097e-01, -2.5848e-01,  3.1161e-01,  3.9392e-01, -3.5412e-01,\n",
      "        -1.9400e-01, -1.5293e-01, -2.4102e-01, -3.0125e-02,  3.7008e-01,\n",
      "         2.2403e-01,  7.8257e-02, -2.6900e-01, -3.4572e-01, -2.6425e-03,\n",
      "         1.7713e-02, -1.1895e-01,  4.3233e-02, -8.8319e-02,  4.5462e-03,\n",
      "        -2.5439e-01,  1.4728e-01,  4.8581e-01,  4.9114e-02, -1.8314e-01,\n",
      "        -5.9602e-02,  1.2808e-01, -5.5221e-02, -2.2270e-01, -3.5185e-01,\n",
      "         9.6984e-02,  1.9694e-01, -4.9658e-02, -2.3089e-01,  9.6932e-02,\n",
      "        -5.9364e-02, -1.0977e-01, -1.0774e+00, -1.7037e-01, -5.0538e-01,\n",
      "        -1.3686e-01, -4.6037e-01,  1.6326e-01,  5.6601e-01,  6.3967e-02,\n",
      "        -2.1306e-01, -1.4903e-01,  2.7365e-01,  7.4819e-02, -5.1876e-03,\n",
      "        -5.4667e-02,  4.2225e-01, -1.0746e-01,  3.5039e-02, -3.6283e-01,\n",
      "         4.7700e-02, -3.4952e-02,  2.4087e-01, -1.5248e-02, -1.0470e-01,\n",
      "        -2.1243e-01, -5.1084e-01,  3.7812e-02, -2.0369e-01,  8.7614e-03,\n",
      "        -5.5485e-03, -1.4413e-02, -1.5388e-01,  2.1188e-01,  6.3782e-02,\n",
      "         2.0168e-01, -4.1447e-01,  1.8987e-01,  1.7409e-01,  1.5857e-01,\n",
      "         1.0428e-02, -2.3079e-01, -2.0696e-01,  1.5372e-01, -1.3400e-01,\n",
      "        -6.5152e-02, -9.0121e-02,  1.3587e-01, -1.1827e-01, -2.9220e-02,\n",
      "         9.5783e-02,  6.6439e-02, -2.1315e-02, -2.3742e-01, -1.7472e-01,\n",
      "        -7.9916e-01,  4.4483e-02,  2.8404e-01, -2.2308e-02, -5.6764e-02,\n",
      "         1.9825e-01, -2.0278e-01,  3.3376e-01, -5.7932e-02, -2.7878e-01,\n",
      "         4.6812e-01,  2.6236e-01, -9.5878e-02, -1.5614e-01, -1.8900e-01,\n",
      "        -4.5504e-02, -1.4836e-01, -7.0388e-02, -2.5256e-01, -3.6207e-02,\n",
      "        -2.3713e-01,  1.1566e-01, -6.3016e-02, -1.5543e-01, -1.6353e-01,\n",
      "         1.1100e-01, -5.8401e-02, -4.0934e-01, -5.6329e-02, -1.1422e-01,\n",
      "         4.5686e-01,  1.9147e-01, -2.4061e+00, -1.3890e-01,  4.9572e-02,\n",
      "         1.0699e-01,  1.8370e-01, -4.5469e-02, -8.1192e-02, -1.1795e-01,\n",
      "        -1.3250e-01, -3.6430e-02,  2.8795e-01, -7.0326e-02,  9.9012e-02,\n",
      "         7.5754e-02, -8.5289e-02, -2.4367e-02], device='cuda:0')\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agora temos um vetor de 768 recursos que representam nossa sentença de entrada**. Mas podemos fazer mais! O artigo do BERT discute como alcançar os melhores resultados concatenando a saída das últimas quatro camadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso exemplo, isso significa que precisamos pegar as últimas quatro camadas de hidden_states, concatená-los e gerar a média. Nós queremos concatenar no eixo das dimensões ocultas de `768`. Como consequência, nosso vetor de saída concatenado irá ser do tamanho `(1, 7, 3072)` onde `3072 = 4 * 768`, ou seja, a concatenação de quatro camadas com uma dimensão oculta de 768. O vetor concatenado é muito maior do que a saída de apenas uma camada, o que significa que contém muito mais recursos.\n",
    "\n",
    "Para algumas tarefas, esses recursos `3072` podem tem um desempenho melhor do que `768`.\n",
    "\n",
    "Tendo um vetor de forma `(1, 7, 3072)`, ainda precisamos obter a média sobre a dimensão do token, como fizemos antes, ficando com um vetor de recurso de tamanho `(3072,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 3072])\n",
      "tensor([ 0.2750,  0.1831, -0.0887,  ...,  0.2894, -0.0034,  0.0764],\n",
      "       device='cuda:0')\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "# obter as ultimas quatro camadas\n",
    "last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "# juntas as camadas em uma tupla e concatenar com a ultima dimensão\n",
    "cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "print(cat_hidden_states.size())\n",
    "\n",
    "# pegar a média do vetor concatenado sobre a dimensão do token\n",
    "cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "print(cat_sentence_embedding)\n",
    "print(cat_sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando e carregando resultados\n",
    "\n",
    "É possível usar o vetor de recurso gerado em outro modelo ou tarefa, para isso basta salvar o tensor com `torch.save` e carregá-lo em outro script com `torch.load`, gerando arquivos na extensão .pt (PyTorch). Não é possível ler o arquivo salvo com um editor de texto (é um objeto especial que permite uma des(compressão) eficiente).\n",
    "\n",
    "Também é possível salvar os tensores em um formato legível, convertendo em numpy e use algo como `np.savetxt` `('tensor.txt', your_tensor.numpy ())`, porém essa abordagem não é recomendada (é melhor usar o torch.save ou outra técnica de compressão).\n",
    "\n",
    "Ao usar `.cpu ()`, dizemos ao PyTorch que queremos mover o tensor de saída de volta da GPU para a CPU. Isso não é obrigatório, mas é uma boa prática, ao fazer extração de recursos, mover os dados para a CPU. Desta forma, ao carregá-lo, ele é carregado como um tensor de CPU em vez de um tensor CUDA. Depois podemos mover novamente para a GPU, se necessário, mas usar a CPU por padrão é uma boa ideia (o tensor deve estar na CPU para podermos convertê-lo para `.numpy () )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2750,  0.1831, -0.0887,  ...,  0.2894, -0.0034,  0.0764])\n",
      "torch.Size([3072])\n",
      "[ 0.2749734   0.1831336  -0.08865138 ...  0.28939357 -0.00340417\n",
      "  0.07635488]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# sava nossa representação de sentença \n",
    "torch.save(cat_sentence_embedding.cpu(), 'my_sent_embed.pt')\n",
    "\n",
    "# faz o load\n",
    "loaded_tensor = torch.load('my_sent_embed.pt')\n",
    "print(loaded_tensor)\n",
    "print(loaded_tensor.size())\n",
    "\n",
    "# converte para numpy para usar por ex com sklearn\n",
    "np_loaded_tensor = loaded_tensor.numpy()\n",
    "print(np_loaded_tensor)\n",
    "print(type(np_loaded_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
